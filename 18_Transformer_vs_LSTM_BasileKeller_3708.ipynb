{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 117,
      "id": "dbc49d9b-5f41-4169-bbda-5d77f285459f",
      "metadata": {
        "id": "dbc49d9b-5f41-4169-bbda-5d77f285459f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Disable tensorflow debugging logs\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "953c4517-86bd-4ed3-836a-f7356c895317",
      "metadata": {
        "tags": [],
        "id": "953c4517-86bd-4ed3-836a-f7356c895317"
      },
      "source": [
        "## Importar dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a58d62a",
      "metadata": {
        "id": "8a58d62a",
        "outputId": "a15ec353-1dd0-4f67-90d4-39fd42a65d51"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The script tqdm.exe is installed in 'C:\\Users\\rodol\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script f2py.exe is installed in 'C:\\Users\\rodol\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script google-oauthlib-tool.exe is installed in 'C:\\Users\\rodol\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\rodol\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The scripts estimator_ckpt_converter.exe, import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\\Users\\rodol\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script tfds.exe is installed in 'C:\\Users\\rodol\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-text 2.10.0 requires tensorflow<2.11,>=2.10.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.12.0 which is incompatible.\n",
            "tensorflow-gpu 2.10.0 requires keras<2.11,>=2.10.0, but you have keras 2.12.0 which is incompatible.\n",
            "tensorflow-gpu 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.\n",
            "tensorflow-gpu 2.10.0 requires tensorboard<2.11,>=2.10, but you have tensorboard 2.12.3 which is incompatible.\n",
            "tensorflow-gpu 2.10.0 requires tensorflow-estimator<2.11,>=2.10.0, but you have tensorflow-estimator 2.12.0 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q tensorflow-datasets tensorflow --user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "id": "3404f6ab-2dd4-4c7e-989a-ea33ee8c14f9",
      "metadata": {
        "id": "3404f6ab-2dd4-4c7e-989a-ea33ee8c14f9"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "342afe6a-fe93-4832-b573-96f8e209cc1c",
      "metadata": {
        "id": "342afe6a-fe93-4832-b573-96f8e209cc1c"
      },
      "outputs": [],
      "source": [
        "dataset = tfds.load('imdb_reviews', as_supervised=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "id": "b155f67f-344e-4841-adab-3a2e58608756",
      "metadata": {
        "id": "b155f67f-344e-4841-adab-3a2e58608756"
      },
      "outputs": [],
      "source": [
        "raw_train_ds, raw_test_ds = dataset['train'], dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "id": "76dae875-74f1-46d0-a402-03cf4bdd4681",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76dae875-74f1-46d0-a402-03cf4bdd4681",
        "outputId": "6ac9f9f2-aa15-4f71-d4c8-443eb3a10ab3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\" 0\n"
          ]
        }
      ],
      "source": [
        "for text, label in raw_train_ds.take(1):\n",
        "    print(text.numpy(), label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5b56f4b-0c53-466f-a810-48091062b5af",
      "metadata": {
        "id": "b5b56f4b-0c53-466f-a810-48091062b5af"
      },
      "source": [
        "## Preparar dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "id": "4b8a8a41",
      "metadata": {
        "id": "4b8a8a41"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras import layers \n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "id": "39b8f2a7-f119-48ae-8dba-f8fa7fb03b22",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39b8f2a7-f119-48ae-8dba-f8fa7fb03b22",
        "outputId": "befc146d-f1f4-48f9-d241-87681cd3c11f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ],
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "BUFFER_SIZE = tf.data.experimental.cardinality(raw_train_ds)\n",
        "BUFFER_SIZE.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "b380bb5a-b845-4440-a468-147af7bbcb43",
      "metadata": {
        "id": "b380bb5a-b845-4440-a468-147af7bbcb43"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "voc_size = 5000\n",
        "\n",
        "train_ds = raw_train_ds.shuffle(BUFFER_SIZE).batch(\n",
        "        batch_size, num_parallel_calls=AUTOTUNE).prefetch(\n",
        "        AUTOTUNE)\n",
        "\n",
        "test_ds = raw_test_ds.batch(\n",
        "        batch_size, num_parallel_calls=AUTOTUNE).prefetch(\n",
        "        AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "48972e5d-a421-415c-bbae-c3b1440ea599",
      "metadata": {
        "scrolled": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48972e5d-a421-415c-bbae-c3b1440ea599",
        "outputId": "c4e360f7-4760-46d5-daf8-bdffc0a45e88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'If it were not for the \"Oh So Gourgous,\" Natassia Malthe, this B- movie would not have been worth one sector of my Tivo disk space! In what low rent, back lot warehouse was the supposed space port filmed in? \"Continuity People!\" It\\'s a basic principle in real movie making! By night an alleged space port and by day (night and day on a space station?) a warehouse!??!? People Please! The only thing I will commend this movie for, is the wardrobe dept. for continuously, keeping Natassia in those tight shape revealing outfits! Even the women who saw this bomb had to appreciate the outfits that she obviously spent some time getting into, each day of filming! The Sci-fi channel would have been better off showing SpaceBalls! At least there would have been some real humor in watching something so unbelievable.<br /><br />P.S. Michael Ironside, please Fire Your Agent ASAP! You are so much better of an actor, to be even associated with this level of movie making.', shape=(), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "for text, label in train_ds.take(1):\n",
        "    print(text[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66bae1da-6e63-4d24-828d-3347905e8e10",
      "metadata": {
        "id": "66bae1da-6e63-4d24-828d-3347905e8e10"
      },
      "source": [
        "## Tokenización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "id": "288fb5a9-8fac-40a7-a696-d58adfa6d4e6",
      "metadata": {
        "id": "288fb5a9-8fac-40a7-a696-d58adfa6d4e6"
      },
      "outputs": [],
      "source": [
        "maxlen = 128\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    max_tokens=voc_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=maxlen)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3ee458d-4856-40c4-b3f7-e9adeda67af5",
      "metadata": {
        "id": "a3ee458d-4856-40c4-b3f7-e9adeda67af5"
      },
      "source": [
        "- Adaptar la capa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "id": "42deec01-632e-406a-96d2-bea5dc563572",
      "metadata": {
        "id": "42deec01-632e-406a-96d2-bea5dc563572"
      },
      "outputs": [],
      "source": [
        "vectorize_layer_ds = train_ds.map(lambda text, label: text)\n",
        "vectorize_layer.adapt(vectorize_layer_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "id": "a992d26a-a2a5-43a6-801a-69185baa09da",
      "metadata": {
        "scrolled": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a992d26a-a2a5-43a6-801a-69185baa09da",
        "outputId": "72e6c8d1-844d-45f9-d9f9-104cc77cf697"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it']"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ],
      "source": [
        "vectorize_layer.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dd289ef-b4ea-43f9-b059-4439e4a71888",
      "metadata": {
        "id": "5dd289ef-b4ea-43f9-b059-4439e4a71888"
      },
      "source": [
        "- Probar vectorize_layer con batch de prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "id": "5a5741fa-1582-4479-9697-9aa424da11be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a5741fa-1582-4479-9697-9aa424da11be",
        "outputId": "6825c0a6-705c-4260-d505-db06070c3c3e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 128), dtype=int64, numpy=\n",
              "array([[ 1, 48,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ],
      "source": [
        "test_batch = tf.constant([['Hi there']])\n",
        "vectorize_layer(test_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ea078c7-a54d-40d5-ad6c-57a8e79cc616",
      "metadata": {
        "id": "8ea078c7-a54d-40d5-ad6c-57a8e79cc616"
      },
      "source": [
        "## Definir LSTM\n",
        "\n",
        "RNN:\n",
        "\\begin{equation}\n",
        "h_t = f(Wx_t + Uh_{t-1} + b)\n",
        "\\end{equation}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "210f06d3-2cb7-491e-bb56-30779cdc7e09",
      "metadata": {
        "id": "210f06d3-2cb7-491e-bb56-30779cdc7e09"
      },
      "source": [
        "LSTM:\n",
        "\n",
        "\\begin{align}\n",
        "i_t & = \\sigma(W^ix_t + U^ih_{t-1} + b^i) \\\\\n",
        "f_t & = \\sigma(W^fx_t + U^fh_{t-1} + b^f) \\\\\n",
        "o_t & = \\sigma(W^ox_t + U^oh_{t-1} + b^o) \\\\\n",
        "g_t & = \\text{tanh}(W^gx_t + U^gh_{t-1} + b^g) \\\\\n",
        "c_t & = f_t \\odot c_{t-1} + i_t \\odot g_t\\\\\n",
        "h_t & = o_t \\odot \\text{tanh}(c_t) \\\\\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "id": "59d56968-9a5f-4dec-adaf-a4e7d8f66f91",
      "metadata": {
        "id": "59d56968-9a5f-4dec-adaf-a4e7d8f66f91"
      },
      "outputs": [],
      "source": [
        "lstm = tf.keras.Sequential([\n",
        "    vectorize_layer,\n",
        "    layers.Embedding(\n",
        "        input_dim=voc_size, output_dim=128),\n",
        "    layers.Bidirectional(layers.LSTM(128)),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ea47915-00bb-42f5-81e4-d33d3e37b4be",
      "metadata": {
        "id": "3ea47915-00bb-42f5-81e4-d33d3e37b4be"
      },
      "source": [
        "- Probar LSTM con batch de prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "id": "aa876b8a-c6fa-4c9d-8819-476b70288bc9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa876b8a-c6fa-4c9d-8819-476b70288bc9",
        "outputId": "1993be98-c820-44fc-f695-840ee62a5d64"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.00345709]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ],
      "source": [
        "lstm(test_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddfca56a-eb6d-418c-9b11-4a5300ab9bd1",
      "metadata": {
        "id": "ddfca56a-eb6d-418c-9b11-4a5300ab9bd1"
      },
      "source": [
        "- Información del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "id": "a315c30f-f107-43fa-b5d6-1a4334ff0c3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a315c30f-f107-43fa-b5d6-1a4334ff0c3e",
        "outputId": "37a1f856-bd6d-4bd8-eca5-5f98d5ef2622"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization_1 (TextV  (None, 128)              0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding_23 (Embedding)    (None, 128, 128)          640000    \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 256)              263168    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_82 (Dense)            (None, 64)                16448     \n",
            "                                                                 \n",
            " dense_83 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 919,681\n",
            "Trainable params: 919,681\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "lstm.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78948806-9b9b-4a4b-960b-e3668b750240",
      "metadata": {
        "id": "78948806-9b9b-4a4b-960b-e3668b750240"
      },
      "source": [
        "## Entrenamiento LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "id": "29a2b23f-a780-4b94-b280-5feeecc07ad1",
      "metadata": {
        "id": "29a2b23f-a780-4b94-b280-5feeecc07ad1"
      },
      "outputs": [],
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "id": "1f299f93-f2ac-4964-a2fb-bdfd7668217e",
      "metadata": {
        "id": "1f299f93-f2ac-4964-a2fb-bdfd7668217e"
      },
      "outputs": [],
      "source": [
        "lstm_opt = tf.keras.optimizers.Adam(learning_rate=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "id": "6578dcd0-2eca-4a94-8d6d-2efbba652d92",
      "metadata": {
        "id": "6578dcd0-2eca-4a94-8d6d-2efbba652d92"
      },
      "outputs": [],
      "source": [
        "train_loss_avg = tf.keras.metrics.Mean(name='train_loss')\n",
        "val_loss_avg = tf.keras.metrics.Mean(name='val_loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "id": "a761811b-6555-43e6-ae7a-031c50781aab",
      "metadata": {
        "id": "a761811b-6555-43e6-ae7a-031c50781aab"
      },
      "outputs": [],
      "source": [
        "epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "id": "298716de-4ec4-4b9a-a382-c6ffddc7335b",
      "metadata": {
        "id": "298716de-4ec4-4b9a-a382-c6ffddc7335b"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(text, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = lstm(text, training=True)\n",
        "        loss_value = loss(tf.cast(target, tf.float32), logits)\n",
        "\n",
        "    gradients = tape.gradient(loss_value, lstm.trainable_weights)\n",
        "    lstm_opt.apply_gradients(zip(gradients, lstm.trainable_weights))\n",
        "    train_loss_avg(loss_value)\n",
        "    \n",
        "@tf.function\n",
        "def test_step(text, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = lstm(text, training=False)\n",
        "        loss_value = loss(tf.cast(target, tf.float32), logits)\n",
        "\n",
        "    val_loss_avg(loss_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "id": "8d99446c-6232-47e6-bfee-38cedf9d9389",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d99446c-6232-47e6-bfee-38cedf9d9389",
        "outputId": "0f06fb91-46d9-4942-8bea-934c2c596ae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[1 0 1 1 0 0 0 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1\n",
            " 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0\n",
            " 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 0\n",
            " 0 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 0], shape=(128,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "for text, target in train_ds.take(1):\n",
        "    print(target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "id": "d8d7010c-48b3-49e3-bf55-16c19197b65c",
      "metadata": {
        "id": "d8d7010c-48b3-49e3-bf55-16c19197b65c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceded227-483f-4c05-dc5a-4325e6a95bf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Train loss: 0.6523\n",
            "Val loss: 0.5014\n",
            "Epoch: 1 Train loss: 0.4057\n",
            "Val loss: 0.3892\n",
            "Epoch: 2 Train loss: 0.3279\n",
            "Val loss: 0.3821\n",
            "Epoch: 3 Train loss: 0.2923\n",
            "Val loss: 0.3806\n",
            "Epoch: 4 Train loss: 0.2724\n",
            "Val loss: 0.3899\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    for text, target in train_ds:\n",
        "        train_step(text, target)\n",
        "        \n",
        "    print(f'Epoch: {epoch} Train loss: {train_loss_avg.result():.4f}')\n",
        "    train_loss_avg.reset_states()\n",
        "    \n",
        "    for text, target in test_ds:\n",
        "        test_step(text, target)\n",
        "        \n",
        "    print(f'Val loss: {val_loss_avg.result():.4f}')\n",
        "    val_loss_avg.reset_states()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a0022f0-1275-4fae-9339-efc3e2001bd1",
      "metadata": {
        "id": "8a0022f0-1275-4fae-9339-efc3e2001bd1"
      },
      "source": [
        "## Definir Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5fec4fb-fc1f-43c1-adb2-f0bd9e93be19",
      "metadata": {
        "id": "c5fec4fb-fc1f-43c1-adb2-f0bd9e93be19"
      },
      "source": [
        "<img src=\"../img/dot_product.png\" width=\"500\"/>\n",
        "\n",
        "__Imagen tomada de Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.__\n",
        "\n",
        "\\begin{equation}\n",
        "\\mbox{MultiHead}(Q, K, V) = \\text{Concat}(\\mbox{head}_1,\\mbox{head}_2,\\ldots,\\mbox{head}_h)W^O,\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\mbox{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) = \\text{softmax}\\left[\\frac{QW_i^Q(KW_i^K)^T}{\\sqrt{d_k}}\\right]VW_i^V,\n",
        "\\label{eq:selfattention}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "id": "6354d4e4-52cd-45d2-bc30-7738b52a30f3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6354d4e4-52cd-45d2-bc30-7738b52a30f3",
        "outputId": "19bb2b52-627a-4166-bf4c-dc81bb7e8278"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 16, 128), dtype=float32, numpy=\n",
              "array([[[ 1.7492265 ,  0.15875828, -1.2607868 , ...,  0.41106725,\n",
              "         -0.24703759, -1.7047443 ],\n",
              "        [ 1.7492265 ,  0.15875828, -1.2607868 , ...,  0.41106725,\n",
              "         -0.24703759, -1.7047443 ],\n",
              "        [ 1.7492265 ,  0.15875828, -1.2607868 , ...,  0.41106725,\n",
              "         -0.24703759, -1.7047443 ],\n",
              "        ...,\n",
              "        [ 1.7492265 ,  0.15875828, -1.2607868 , ...,  0.41106725,\n",
              "         -0.24703759, -1.7047443 ],\n",
              "        [ 1.7492265 ,  0.15875828, -1.2607868 , ...,  0.41106725,\n",
              "         -0.24703759, -1.7047443 ],\n",
              "        [ 1.7492265 ,  0.15875828, -1.2607868 , ...,  0.41106725,\n",
              "         -0.24703759, -1.7047443 ]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ],
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    def __init__(self, model_dim, n_heads, rate=0.1, initializer='glorot_uniform'):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.model_dim = model_dim\n",
        "\n",
        "        assert model_dim % self.n_heads == 0\n",
        "\n",
        "        self.head_dim = model_dim // self.n_heads\n",
        "\n",
        "        self.wq = layers.Dense(model_dim, kernel_initializer=initializer)\n",
        "        self.wk = layers.Dense(model_dim, kernel_initializer=initializer)\n",
        "        self.wv = layers.Dense(model_dim, kernel_initializer=initializer)\n",
        "        \n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "        \n",
        "        self.wo = layers.Dense(model_dim, kernel_initializer=initializer)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.n_heads, self.head_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, q, k, v):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  \n",
        "        k = self.wk(k)  \n",
        "        v = self.wv(v)  \n",
        "\n",
        "        q = self.split_heads(q, batch_size) \n",
        "        k = self.split_heads(k, batch_size)  \n",
        "        v = self.split_heads(v, batch_size) \n",
        "\n",
        "        # print(q.shape(), k.shape(), v.shape()) #Imprime tamaño matrices\n",
        "\n",
        "        dh = tf.cast(self.head_dim, tf.float32)\n",
        "        qk = tf.matmul(q, k, transpose_b=True)\n",
        "        scaled_qk =  qk / tf.math.sqrt(dh)\n",
        "\n",
        "        attn = self.dropout1(tf.nn.softmax(scaled_qk, axis=-1))\n",
        "        attn = tf.matmul(attn, v) \n",
        "\n",
        "        attn = tf.transpose(attn, perm=[0, 2, 1, 3]) \n",
        "        original_size_attention = tf.reshape(attn, (batch_size, -1, self.model_dim)) \n",
        "\n",
        "        output = self.dropout2(self.wo(original_size_attention))\n",
        "        return output\n",
        "    \n",
        "x = tf.ones([1, 16, 128]) #Longitud de secuencia 16\n",
        "MultiHeadAttention(128,2)(x,x,x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4619cb0d-3f8c-49d1-957b-a8a41a59a952",
      "metadata": {
        "id": "4619cb0d-3f8c-49d1-957b-a8a41a59a952"
      },
      "source": [
        "- Definir embedding de posición"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "id": "432249e4-1667-41d6-bb0b-9f218f02dfd1",
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "432249e4-1667-41d6-bb0b-9f218f02dfd1"
      },
      "outputs": [],
      "source": [
        "class TokenEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, emb_dim, \n",
        "                 rate=0.0):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.max_len = maxlen\n",
        "        self.token_emb = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=emb_dim)\n",
        "        self.position_emb = layers.Embedding(\n",
        "            input_dim=maxlen, output_dim=emb_dim)\n",
        "        self.dropout = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x):\n",
        "        token_embeddings = self.token_emb(x)\n",
        "        positions = tf.range(start=0, limit=self.max_len, delta=1)\n",
        "        positions = self.position_emb(positions)\n",
        "        return self.dropout(token_embeddings + positions) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "id": "7d7859bf-5847-41b0-92a1-aabca46476a2",
      "metadata": {
        "id": "7d7859bf-5847-41b0-92a1-aabca46476a2"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, model_dim, n_heads=2, mlp_dim=512, \n",
        "                 rate=0.0, eps=1e-6):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attn = MultiHeadAttention(model_dim, n_heads, rate)\n",
        "        self.mlp = tf.keras.Sequential([\n",
        "            layers.Dense(mlp_dim, activation='gelu'), \n",
        "            layers.Dense(model_dim),\n",
        "            layers.Dropout(rate)\n",
        "        ])\n",
        "        self.ln1 = layers.LayerNormalization(epsilon=eps)\n",
        "        self.ln2 = layers.LayerNormalization(epsilon=eps)\n",
        "        self.drop1 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):  \n",
        "        x = self.drop1(self.attn(inputs, inputs, inputs), training=training) \n",
        "        x = self.ln1(x + inputs)\n",
        "        return self.ln2(self.mlp(x) + x)\n",
        "    \n",
        "block = TransformerBlock(128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "id": "673129bd-0046-4da4-98a6-d3f6d43f9f92",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "673129bd-0046-4da4-98a6-d3f6d43f9f92",
        "outputId": "06c90de9-9a90-41a6-c159-c9fefe8f0b6c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.4275923]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ],
      "source": [
        "class Transformer(tf.keras.models.Model):\n",
        "    def __init__(self, model_dim, voc_size, mlp_dim=256, \n",
        "                 maxlen=128, heads=4):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.emb = TokenEmbedding(maxlen, voc_size, model_dim, rate=0.25)\n",
        "        self.block = TransformerBlock(model_dim, heads, mlp_dim, rate=0.2)\n",
        "        self.out = tf.keras.Sequential([\n",
        "            layers.GlobalAveragePooling1D(),\n",
        "            layers.Dense(1)\n",
        "        ])\n",
        "    \n",
        "    def call(self, x):\n",
        "        x = vectorize_layer(x)\n",
        "        x = self.emb(x)\n",
        "        x = self.block(x)\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "    \n",
        "transformer = Transformer(512, voc_size, heads = 8)\n",
        "transformer(test_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "id": "e426a166-bacb-4e22-a2f5-b6208621308c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e426a166-bacb-4e22-a2f5-b6208621308c",
        "outputId": "807fa70a-8cdc-4269-f095-3cdc24a50257"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " token_embedding_11 (TokenEm  multiple                 2625536   \n",
            " bedding)                                                        \n",
            "                                                                 \n",
            " transformer_block_13 (Trans  multiple                 1315584   \n",
            " formerBlock)                                                    \n",
            "                                                                 \n",
            " sequential_25 (Sequential)  (1, 1)                    513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,941,633\n",
            "Trainable params: 3,941,633\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "transformer.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08a1358c-38e6-491b-8489-cfb9e1de8b52",
      "metadata": {
        "id": "08a1358c-38e6-491b-8489-cfb9e1de8b52"
      },
      "source": [
        "## Entrenamiento Transformer\n",
        "- Utilizar los mismos parámteros de LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "id": "370c0112-887f-4c32-b364-04c05ae68195",
      "metadata": {
        "id": "370c0112-887f-4c32-b364-04c05ae68195"
      },
      "outputs": [],
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "id": "ba85bf54-db2b-46dc-b4bc-bf599b2c4677",
      "metadata": {
        "id": "ba85bf54-db2b-46dc-b4bc-bf599b2c4677"
      },
      "outputs": [],
      "source": [
        "trans_opt = tf.keras.optimizers.Adam(learning_rate=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "id": "05f86685-9fba-4c5e-8906-d3a5f43877f7",
      "metadata": {
        "id": "05f86685-9fba-4c5e-8906-d3a5f43877f7"
      },
      "outputs": [],
      "source": [
        "train_loss_avg = tf.keras.metrics.Mean(name='train_loss')\n",
        "val_loss_avg = tf.keras.metrics.Mean(name='val_loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "id": "56b431b6-dc3a-4245-8060-d41898c61bdc",
      "metadata": {
        "id": "56b431b6-dc3a-4245-8060-d41898c61bdc"
      },
      "outputs": [],
      "source": [
        "epochs = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "id": "5f2ac08c-0a6b-48d1-89d5-be0b6c3933b6",
      "metadata": {
        "id": "5f2ac08c-0a6b-48d1-89d5-be0b6c3933b6"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(text, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = transformer(text, training=True)\n",
        "        loss_value = loss(tf.cast(target, tf.float32), logits)\n",
        "\n",
        "    gradients = tape.gradient(loss_value, transformer.trainable_weights)\n",
        "    trans_opt.apply_gradients(zip(gradients, transformer.trainable_weights))\n",
        "    train_loss_avg(loss_value)\n",
        "    \n",
        "@tf.function\n",
        "def test_step(text, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = transformer(text, training=False)\n",
        "        loss_value = loss(tf.cast(target, tf.float32), logits)\n",
        "\n",
        "    val_loss_avg(loss_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "id": "7d2a6269-eee4-408a-b358-654c8078efd7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d2a6269-eee4-408a-b358-654c8078efd7",
        "outputId": "88d2b8b5-414f-4ef3-d019-5692933db0a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Train loss: 0.5326\n",
            "Val loss: 0.4240\n",
            "Epoch: 1 Train loss: 0.3606\n",
            "Val loss: 0.3708\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    for text, target in train_ds:\n",
        "        train_step(text, target)\n",
        "        \n",
        "    print(f'Epoch: {epoch} Train loss: {train_loss_avg.result():.4f}')\n",
        "    train_loss_avg.reset_states()\n",
        "    \n",
        "    for text, target in test_ds:\n",
        "        test_step(text, target)\n",
        "        \n",
        "    print(f'Val loss: {val_loss_avg.result():.4f}')\n",
        "    val_loss_avg.reset_states()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65c48752-cf76-48ac-997d-8f245d257811",
      "metadata": {
        "id": "65c48752-cf76-48ac-997d-8f245d257811"
      },
      "source": [
        "## Ejercicio"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d1df65d-ba82-4850-b59b-768a3cc8ff51",
      "metadata": {
        "id": "0d1df65d-ba82-4850-b59b-768a3cc8ff51"
      },
      "source": [
        "- Modificar los hiperparámetros de los modelos para obtener mejores resultados.\n",
        "- Modificar las arquitecturas, comparar resultados con GRU.\n",
        "- Agregar y modificar regularización."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tamaño Embedding, perceptrón multicapa, dropout, profundidad."
      ],
      "metadata": {
        "id": "C_MooVxqDW-T"
      },
      "id": "C_MooVxqDW-T"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}