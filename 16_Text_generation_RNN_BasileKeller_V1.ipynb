{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "316fd0ae-ef28-4452-8b3a-ba304e0e811b",
   "metadata": {},
   "source": [
    "- Game of thrones book: https://www.kaggle.com/datasets/khulasasndh/game-of-thrones-books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a667cc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-text\n",
      "  Using cached tensorflow_text-2.10.0-cp310-cp310-win_amd64.whl (5.0 MB)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorflow-text) (0.13.0)\n",
      "Collecting tensorflow<2.11,>=2.10.0\n",
      "  Using cached tensorflow-2.10.1-cp310-cp310-win_amd64.whl (455.9 MB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (1.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (3.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (1.16.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (0.4.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (3.8.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (3.19.6)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (1.15.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (2.10.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (1.24.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (66.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (0.31.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (1.1.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (23.1)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (2.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (4.5.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (2.3.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (1.54.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (16.0.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (2.10.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text) (23.5.9)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.11,>=2.10.0->tensorflow-text) (0.38.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (3.4.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (2.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (2.30.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (2.18.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (0.3.0)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (1.26.15)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (5.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\rodol\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text) (3.2.2)\n",
      "Installing collected packages: tensorflow, tensorflow-text\n",
      "Successfully installed tensorflow-2.10.1 tensorflow-text-2.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fccba23-1590-4a3a-95ee-bf84561cf77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Disable tensorflow debugging logs\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5aff8a1c-af29-48ae-8ed0-c40b5b1b31f9",
   "metadata": {},
   "source": [
    "- Convertir documento a minúsculas para reducir el tamaño del vocabulario y obtener número de palabras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c897b75-d93a-4ccb-845e-09aa0c0ce000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: 1628063\n"
     ]
    }
   ],
   "source": [
    "path = './001ssb.txt'\n",
    "book = open(path, 'rb').read().decode(encoding='utf-8').lower()\n",
    "\n",
    "print(f'Words: {len(book)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8de59a7-c746-49dd-8237-83c9456df51e",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "- Preprocesamiento del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38314433-fdc0-4ed5-9744-ff9382ce9b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'A', b'game', b'of', b'thrones', b',', b'jon', b'and', b'sansa', b'.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tf_text.UnicodeScriptTokenizer()\n",
    "tokens = tokenizer.tokenize([\"A game of thrones, jon and sansa.\"]).to_list()\n",
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a358f2fb-6dc5-44ac-8dec-d1ac3c1591be",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'a',\n",
       " b'game',\n",
       " b'of',\n",
       " b'thrones',\n",
       " b'book',\n",
       " b'one',\n",
       " b'of',\n",
       " b'a',\n",
       " b'song',\n",
       " b'of']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_words =  tokenizer.tokenize([book]).to_list()[0]\n",
    "book_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48036c9e-983a-4f25-a15c-94c110f3327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_ds = tf.data.Dataset.from_tensor_slices(book_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddc21451-450e-443c-9e49-c0e832f3734a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'a'\n",
      "b'game'\n",
      "b'of'\n",
      "b'thrones'\n",
      "b'book'\n",
      "b'one'\n",
      "b'of'\n",
      "b'a'\n",
      "b'song'\n",
      "b'of'\n",
      "b'ice'\n",
      "b'and'\n",
      "b'fire'\n",
      "b'by'\n",
      "b'george'\n",
      "b'r'\n",
      "b'.'\n",
      "b'r'\n",
      "b'.'\n",
      "b'martin'\n"
     ]
    }
   ],
   "source": [
    "for words in words_ds.take(20):\n",
    "    print(words.numpy())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f76da24-1ddd-4e95-a897-f82b51784146",
   "metadata": {},
   "source": [
    "- Generar lotes de oraciones y definir longitud de secuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dbe1fbf-d05a-41e5-aac1-fad0663172d6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'a' b'game' b'of' b'thrones' b'book' b'one' b'of' b'a' b'song' b'of'\n",
      " b'ice' b'and' b'fire' b'by' b'george' b'r' b'.' b'r' b'.' b'martin'\n",
      " b'prologue' b'\"' b'we' b'should' b'start' b'back' b',\"' b'gared' b'urged'\n",
      " b'as' b'the' b'woods' b'began' b'to' b'grow' b'dark' b'around' b'them'\n",
      " b'.\"' b'the' b'wildlings' b'are' b'dead' b'.\"\"' b'do' b'the' b'dead'\n",
      " b'frighten' b'you' b'?\"' b'ser']\n"
     ]
    }
   ],
   "source": [
    "seq_length = 50\n",
    "words_batches = words_ds.batch(seq_length+1, \n",
    "                               drop_remainder=True)\n",
    "\n",
    "for words in words_batches.take(1):\n",
    "    print(words.numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6be5bac8-9231-4875-862a-baa284494425",
   "metadata": {},
   "source": [
    "- Utiliza __join__ para que cada tensor del batch sea una sola cadena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38ecd4b6-410a-4151-abb5-59704830a2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_strings(tokens):\n",
    "    text = tf.strings.reduce_join(tokens, axis=0, separator=' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d72d5714-b410-4a31-a634-7239f4b35b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_ds = words_batches.map(join_strings)\n",
    "batch_size = 32\n",
    "BUFFER_SIZE = len(raw_train_ds)\n",
    "\n",
    "raw_train_ds = (\n",
    "    raw_train_ds\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(batch_size, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b91913fa-cfda-46d1-850e-ee4b03605dfb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'. only me ?\"\" you and the child ,\" ser jorah said , grim .\" no . he cannot have my son .\" she would not weep , she decided . she would not shiver with fear . the usurper has woken the dragon now , she told herself ... and'\n",
      " b\"limbs from the greater , and the thickest i 9 straightest branches they could find . they laid the wood east to west , from sunrise to sunset . on the platform they piled khal drogo ' s treasures : his great tent , his painted vests , his saddles and\"\n",
      " b'cut down the faces and gave them to the fire . horrorstruck , the children went to war . the old page 497 songs say that the greenseers used dark magics to make the seas rise and sweep away the land , shattering the arm , but it was too late'\n",
      " b'will want to hunt . i shall send jory south with an honor guard to meet them on the kingsroad and escort them back . gods , how are we going to feed them all ? on his way already , you said ? damn the man . damn his royal'\n",
      " b'returning , my lord .\" page 164\" too long ,\" yoren said .\" most like he \\' s dead .\"\" my uncle is not dead ,\" robb stark said loudly , anger in his tones . he rose from the bench and laid his hand on the hilt of his sword'\n",
      " b'behind them . the waycastle called sky was no more than a high , crescent - shaped wall of unmortared stone raised against the side of the mountain , but even the topless towers of valyria could not have looked more beautiful to catelyn stark . here at last the snow'\n",
      " b'offer him whatever it takes . i will give you a letter to place into the hand of lord stannis baratheon . no one else . not his steward , nor the captain of his guard , nor his lady wife , but only lord stannis himself .\"\" as you command'\n",
      " b'and was making for harrenhal . and there were two kings in the realm . two kings , and no agreement . many of the lords bannermen wanted to march on harrenhal at once , to meet lord tywin and end lannister power for all time . young , hot -'\n",
      " b'to you ,\" he said nervously . one look , and bran knew they were neither foresters nor farmers . he was suddenly conscious of how richly he was dressed . his surcoat was new , dark grey wool with silver buttons , and a heavy silver pin fastened his fur'\n",
      " b\". you saw that , did you ? fool boy , he had no business riding in this company . no money , no squire , no one to help him with that armor . that gorget wasn ' t fastened proper . you think gregor didn ' t notice that\"\n",
      " b'with the scents of garlic and pepper . young squires hurried about on errands as their masters woke , yawning and stretching , to meet the day . a serving man with a goose under his arm bent his knee when he caught sight of them .\" ni \\' lords ,\"'\n",
      " b'. the anger was leaving him as suddenly as it had come .\" this khal drogo is said to have a hundred thousand men in his horde . what would jon say to that ?\"\" he would say that even a million dothraki are no threat to the realm , so'\n",
      " b'same thing , yes ,\" the man said .\" i see a man who would sooner die than betray his king .\"\" he betrayed one already , or have you forgotten ?\" the woman said .\" oh , i don \\' t deny he \\' s loyal to robert , that'\n",
      " b'. yet the words came .\" let my brother walk behind us back to the khalasar .\" among the dothraki , the man who does not ride was no man at all , the lowest of the low , without honor or pride .\" let everyone see him as he is'\n",
      " b'slow descent behind them .\" i see an axe on his saddle , a dirk at his belt , and a sellsword that trails after him like a hungry shadow . where are the chains , sweet one ?\" catelyn shifted uneasily in her seat .\" the dwarf is here ,'\n",
      " b\"of umar lay in a pool of congealing blood , his arm gone at the elbow , a dozen of his moon brothers sprawled around him . shagga was slumped beneath a tree , riddled with arrows , conn ' s head in his lap . tyrion thought they were both\"\n",
      " b'of the head . she stumbled against the table and fell hard , yet cersei lannister did not cry out . her slender fingers brushed her page 286 cheek , where the pale smooth skin was already reddening . on the morrow the bruise would cover half her face .\" i'\n",
      " b'with bright golden eyes , and he ruffled her thick grey fur . shortly , jory brought him ice . when it was over , he said ,\" choose four men and have them take the body north . bury her at winterfell .\"\" all that way ?\" jory said ,'\n",
      " b'broad in his youth , portly as he grew older . now he seemed shrunken , the muscle and meat melted off his bones . even his face sagged . the last time catelyn had seen him , his hair and beard had been brown , well streaked with grey .'\n",
      " b'shook his head , gathered up a fistful of corn , and whistled . the raven flew to his shoulder , crying ,\" live ! live !\" jon ran down the stairs , a smile on his face and robb \\' s letter in his hand .\" my brother is going'\n",
      " b'had dared to put in her letter . she might have the very proof that ned needed to bring the lannisters to ruin , and if it came to war , they would need the arryns and the eastern lords who owed them service . yet the mountain road was perilous'\n",
      " b\". chiggen ' s sword raked across the naked face of a mailed rider , and bronn plunged through the clansmen like a whirlwind , cutting down foes right and left . ser rodrik hammered at the big man in the shadowskin cloak , their horses dancing round each other as\"\n",
      " b'with the scent of spices , pinchfire and sweet lemon and cinnamon . they were escorted across the entry hall , where a mosaic of colored glass depicted the doom of valyria . oil burned in black iron lanterns all along the walls . beneath an arch of twining stone leaves'\n",
      " b'of them had sent the footpad to silence the stark boy , and whether they had truly conspired at the death of lord arryn . if the old hand had been murdered , it was deftly and subtly done . men of his age died of sudden illness all the time'\n",
      " b'bloodflies had settled on his body , though he did not seem to feel them . dany brushed them away and knelt beside him . his eyes were wide open but did not see , and she knew at once that he was blind . when she whispered his name ,'\n",
      " b'it turned out the purple flowers were called poison kisses , and arya got a rash on her arms . sansa would have thought that might have taught her a lesson , but arya laughed about it , and the next day she rubbed mud all over her arms like some'\n",
      " b'his pavilion , lord tywin took his evening meal with his chief knights and lords bannermen , his great crimson - and - gold standard waving overhead from a lofty pike . tyrion arrived late , saddlesore , and sour , all too vividly aware of how amusing he must look'\n",
      " b'one . at first it had not come easy . the khalasar had broken camp the morning after her wedding , moving east toward vaes dothrak , and by the third day dany thought she was going to die . saddle sores opened on her bottom , hideous and bloody .'\n",
      " b'when you are ready . inform jory , but tell no one else , and do nothing until the girls and i have gone . the castle is full of eyes and ears , and i would rather my plans were not known .\"\" as you command , my lord .\"'\n",
      " b'. littlefinger mounted up beside him . jory and the others followed .\" chataya runs a choice establishment ,\" littlefinger said as they rode .\" i \\' ve half a mind to buy it . brothels are a much sounder investment than ships , i \\' ve found . whores seldom'\n",
      " b'toll ? what does he want ?\" she smiled .\" that is what we must discover .\"\" and what if i do not choose to pay this toll ?\"\" then you had best retreat back to moat cailin , deploy to meet lord tywin in battle ... or grow wings .'\n",
      " b'torches in hand . i was sleeping in the west camp , between the rivers . when we heard the fighting and saw the tents being fired , lord brax led us to the rafts and we tried to pole across , but the current pushed us downstream and the tullys'], shape=(32,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for batch in raw_train_ds.take(1):\n",
    "    print(batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5bd836c-9bae-4d0a-ba59-69db03cd7ae9",
   "metadata": {},
   "source": [
    "- Definir tamaño de vocabulario y __vectorize_layer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bf228aa-9e3c-4422-a738-78f1380cb7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size = 11994\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=None,\n",
    "    max_tokens=voc_size - 1,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=seq_length + 1,\n",
    "    #split='character'\n",
    ")\n",
    "\n",
    "vectorize_layer.adapt(raw_train_ds)\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc1d5d3a-fd63-4d4a-99fd-9ddf4d740e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11993"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fec7f6e-d550-4b0d-8901-d3500fabf479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 51), dtype=int64, numpy=\n",
       "array([[   8, 1115,    9,   77,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0],\n",
       "       [   9, 1736,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0]], dtype=int64)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer(['a game of tyrion', 'of thrones'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca673882-4178-436b-bc48-50bb6d848163",
   "metadata": {},
   "source": [
    "- Tokenizar palabras y obtener el texto objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31d53600-2263-44bd-9ed2-4d30427d42b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_target(text):\n",
    "    tokenized_text = vectorize_layer(text)\n",
    "    input_text = tokenized_text[:, :-1]\n",
    "    target_text = tokenized_text[:, 1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67152440-a6cb-4f45-93e0-be22c666a479",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = raw_train_ds.map(get_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c59be795-a130-47a7-ae5d-effec8a0363b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 50) (32, 50)\n",
      "tf.Tensor(\n",
      "[  7   3 206 158  17 107  90  40   2  23  20  90 819  20 145  26 170   6\n",
      "  20  12 153 168   7 447   7  49   2  18 106  20  51   4 167  14  18 168\n",
      "   7  91  14   2 397   4  50  28   3 206   2 121 319 207], shape=(50,), dtype=int64) tf.Tensor(\n",
      "[  3 206 158  17 107  90  40   2  23  20  90 819  20 145  26 170   6  20\n",
      "  12 153 168   7 447   7  49   2  18 106  20  51   4 167  14  18 168   7\n",
      "  91  14   2 397   4  50  28   3 206   2 121 319 207   2], shape=(50,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for input_batch, target_batch in train_ds.take(1):\n",
    "    print(input_batch.shape, target_batch.shape)\n",
    "    print(input_batch[0], target_batch[0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "238c3e71-f3b9-4503-be11-4fc80f56a114",
   "metadata": {},
   "source": [
    "## Definir modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e8de9d6-1294-494d-ba6e-c61d65e0cdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 256\n",
    "model_dim = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8daa37ac-29c2-42ba-ac7d-2c51817ba827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(tf.keras.Model):\n",
    "    def __init__(self, voc_size, emb_dim, model_dim):\n",
    "        super().__init__(self)\n",
    "        self.embedding = layers.Embedding(voc_size, emb_dim)\n",
    "        self.gru = layers.GRU(model_dim,\n",
    "                              return_sequences=True,\n",
    "                              return_state=True)\n",
    "        self.logits = layers.Dense(voc_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.logits(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x \n",
    "\n",
    "model = RNN(voc_size=voc_size,\n",
    "            emb_dim=emb_dim,\n",
    "            model_dim=model_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74b70b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(tf.keras.Model):\n",
    "    def __init__(self, voc_size, emb_dim, model_dim):\n",
    "        super().__init__(self)\n",
    "        self.embedding = layers.Embedding(voc_size, emb_dim)\n",
    "        self.lstm = layers.LSTM(model_dim,    # Se modificó aquí para LSTM\n",
    "                                return_sequences=True,\n",
    "                                return_state=True)\n",
    "        self.logits = layers.Dense(voc_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.lstm.get_initial_state(x) # Se modificó aquí para LSTM\n",
    "        x, state_h, state_c = self.lstm(x, initial_state=states, training=training) # Se modificó aquí para LSTM\n",
    "        x = self.logits(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, [state_h, state_c] # Se modificó aquí para LSTM\n",
    "        else:\n",
    "            return x \n",
    "\n",
    "model = RNN(voc_size=voc_size,\n",
    "            emb_dim=emb_dim,\n",
    "            model_dim=model_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b766f872-c87b-4daa-ada6-5c10aa9fb0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 50, 11994) (32, 50)\n"
     ]
    }
   ],
   "source": [
    "for input_batch, target_batch in train_ds.take(1):\n",
    "    predictions = model(target_batch)\n",
    "    print(predictions.shape, target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c82bbc39-47eb-4cba-8e9a-068cac936364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"rnn_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     multiple                  3070464   \n",
      "                                                                 \n",
      " lstm (LSTM)                 multiple                  5246976   \n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  12293850  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,611,290\n",
      "Trainable params: 20,611,290\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8690a5f9-0afc-4480-b174-df4dbd7bdfd8",
   "metadata": {},
   "source": [
    "- Salida del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "486b3233-c1d5-48d2-af0c-810e1bb1e609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([50, 11994])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7ae5628-d550-4769-84a1-4042b5b62438",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50,), dtype=int64, numpy=\n",
       "array([ 7007,  6175,  4035,   802,  2026,  5543,  1123,   220,  6616,\n",
       "        9159,  8707,  6776,  5084,  8820,  3422, 10065,  7928,  2281,\n",
       "        8820, 10436,  9371,  3104,  8256,  5276,  2310,  3342,  7595,\n",
       "        5472,  8790,  1453,  6960,  7379, 10023,  6271,  4266,  1767,\n",
       "        8481,   100,  2712,  5494, 10403,  1259,  6663,  5364, 10417,\n",
       "        3057,  8608,  9530, 10618, 11482], dtype=int64)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_indices = tf.random.categorical(predictions[0], num_samples=1)\n",
    "pred_indices[:, 0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2df6be19-38b3-4ad7-ba16-5852acf791ba",
   "metadata": {},
   "source": [
    "- Obtener palabras a partir de índices con __vocab__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "986d5838-fad7-4602-9a5b-cbfc0997ad9e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the ox is cooked .\" from the look of it , that might even be before the battle . he walked on . each clan had its own cookfire ; black ears did not eat with stone crows , stone crows did not eat with moon brothers , and no'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([vocab[_] for _ in input_batch[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e16e77f4-e834-492b-bf2b-6d1c3ee1d779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fable squeeze forgets agreed stew flick admitted against necked original retch irritation shutting range snarled furrows tractable sore range dragonglass milled rested splinter outrider muttering warhorse willowisps hat recline memory flocks believing glacier sided toe lower shinguards ; dungeons grimace dwarfed neither meetings lighter driver suspicion sale legacy dacks 538'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([vocab[_] for _ in pred_indices[:, 0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09238979-b7ce-438a-bb48-7cfa96091c2c",
   "metadata": {},
   "source": [
    "## Generación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9dde552e-90ab-447b-b64d-71169d225935",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample(start, model, vectorize_layer, maxlen=500):\n",
    "    states = None\n",
    "    context = tf.constant([start])\n",
    "    output = [start]\n",
    "    for i in range(maxlen):\n",
    "        #print(vectorize_layer(context)[:, :1])\n",
    "        # Obtener solo el primer elemento que regresa vectorize_layer\n",
    "        pred_logits, states = model(vectorize_layer(context)[:, :1], \n",
    "                                    states=states, return_state=True)\n",
    "        #print(pred_logits.shape)\n",
    "        pred_index = tf.random.categorical(pred_logits[:, -1, :], \n",
    "                                           num_samples=1)\n",
    "\n",
    "        #print(vocab[pred_index[0, 0]])\n",
    "        context = tf.constant([vocab[pred_index[0, 0]]])\n",
    "        output.append(vocab[pred_index[0, 0]])\n",
    "\n",
    "    return ' '.join(output)\n",
    "\n",
    "start = 'tyrion'\n",
    "#gen_text = sample(start, model, vectorize_layer)\n",
    "#print(gen_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "504690e6-e7ba-4db2-b328-2edba7362aee",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e5b0e6a-e58d-4217-8712-8b8bce417c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "loss_metric = tf.keras.metrics.Mean(name='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "84bec055-4ba0-4e4f-a81c-5d01f3eb9126",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(input_batch, target_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(input_batch, training=True)\n",
    "        loss_value = loss(target_batch, logits)\n",
    "\n",
    "    gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "    opt.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "    loss_metric(loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f98822e4-a8ed-42e8-b0cb-f8d38be5b351",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38644692-bf3f-4f2a-9812-4c7b9b80b7cd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 7.066257953643799\n",
      "Epoch: 2 Loss: 6.495075225830078\n",
      "Epoch: 3 Loss: 6.41425895690918\n",
      "Epoch: 4 Loss: 6.28152322769165\n",
      "Output: \n",
      "tyrion wedding peered , keep , you hands , longsword , the salute sisters and a dead of do rose on one ,\" pull ,\" the dothraki to firepits of you there to true . foot cheeks to .\" hand were before nothing told . in it else birds age , maester .\" is will there her day . the over jaime them knew down to black across with him of left sky my jon your well him shouting .\" and pulled forever him that followed but as he father , her muttered ,\" nor heart man swayed we must not a roses . she am welled must as no shrug , at head , and his those , dany be inside , your stained , will doing against she had done to still turned forel ' seemed to meet will had spill strange back would to neck sea of pale beside two and come sleep him and landing of mounted . a no ever she commander me .\" all has she center ,\" s sound .\" if back well , bitterly ,\" it you made sun the furnace vaguely easier .\" let and t had whiskers .\" i there did\n",
      "Epoch: 5 Loss: 6.105217933654785\n",
      "Epoch: 6 Loss: 5.955251216888428\n",
      "Epoch: 7 Loss: 5.836941719055176\n",
      "Epoch: 8 Loss: 5.740485668182373\n",
      "Epoch: 9 Loss: 5.6594624519348145\n",
      "Output: \n",
      "tyrion farewell hoster , and to shy rolled close , and twenty that a snows as fifty huddled , old recalling daryn fall .\" sansa would ' knights silent send the arms and a stream man and whatever for called to hand , the day or riding to wed that back , anything this they steamed .\" the old have instant filled and that the earth , the refilled wondering greyjoy grumbled over us as around he told , strange white or say the great tapestries , rubbed more .\" light now , your him as how we said , see a hall and prisoner to horsey crystal down her from banners to promised the wall the king ' s true cloaks and fresh the maester story of then the clangor , morning on the wall of no silver of an greatsword , boy ' s thorne into the jhaqo .\"\" a death from the muscles no banners .\"\" ser she will never , hope at he answered to his long if not said - languid heartsbane choose to winterfell . the hall to the king beneath the fools . she ' s drawbridge to be where fearless , fists ?\"\n",
      "Epoch: 10 Loss: 5.5855302810668945\n",
      "Epoch: 11 Loss: 5.515908718109131\n",
      "Epoch: 12 Loss: 5.447367191314697\n",
      "Epoch: 13 Loss: 5.383866310119629\n",
      "Epoch: 14 Loss: 5.323239803314209\n",
      "Output: \n",
      "tyrion forced to something to the bridge untouched with the nosed beyond away as theon suspiciously of each leaves , and then his hand , strain hands from pentos toward the sword of purple forfeit . he marched at men .\" he was the whip ,\" lord replied of the galloped silver in them dany , seeing it , page bridle year as him , arya . he found her much . i don ' t tell him ? robb shouted ever whose against him that you say so perhaps two one carve his ignorance for sweat , only enjoy a children of handsome than the belly ; the commanded you thousand or bran ' s face in the flagon . i , not him , he could say ride in her arms home on past , and even remember , but the mother was has one constantly down behind she thought at perilous ' s laughter i thought ... ,\" jon pyre shrunken . he had been , the armor of tomard for them on it would power his eyes , louder stark had been , qotho , the wind in the godswood were tie surely all at the hand\n",
      "Epoch: 15 Loss: 5.267168045043945\n",
      "Epoch: 16 Loss: 5.214850902557373\n",
      "Epoch: 17 Loss: 5.1649322509765625\n",
      "Epoch: 18 Loss: 5.11855411529541\n",
      "Epoch: 19 Loss: 5.074267387390137\n",
      "Output: \n",
      "tyrion boomed on his royal grown awhile than at them . there was my far thousand , fierce the gods ' s watch . you saw you glad . in them had have be .\" i have sent down from me . he want .\" come ,\" he loved king , sansa guards upon . you would leave to stay in jory , although he heard his shoulder , but dany must through the last , but she spoke her grass , and a vault ran ; tyrion lifted her coldly , with instruct silence . ned was wide to winterfell . her hear joffrey away , from the shadowskin sweet were tempted . his head page goose sent - around her brother - like skulls . the pyre in his ran themselves in them , protected as his fingers . septa reached back alone to swordplay of him . she mordane knew the horses white black stone , and then catelyn turned before so so one had not seen , illyrio had seen out , and there ... it may close , they did to the fistful of her hand , too turned her silk on his side . and\n",
      "Epoch: 20 Loss: 5.032459735870361\n",
      "Epoch: 21 Loss: 4.993122100830078\n",
      "Epoch: 22 Loss: 4.955480575561523\n",
      "Epoch: 23 Loss: 4.919609069824219\n",
      "Epoch: 24 Loss: 4.8849968910217285\n",
      "Output: \n",
      "tyrion known he could never rape to speak of the litter had with one .\" nymeria washed his throat . they thrust him up from the wind . the serpent ,\" gods luwin ' d , feeling , was and token . hobb despaired of any mouth ' s instant on his half legs .\" what don ' t much , enough to deal amidst you ' s wolf ?\"\" lord in a moment . her brother is a maegi with deal . pray i shall sue . he loved breathlessly the summon some raiders means to lying , and if we rose you was so for jon ? bring soon big flesh outside him , as he could be wet as we have hoarse , dangerous thought , all , and distracted briskly with his throat , and a bruise it would find his hunting ,\" three brothers replied that can fight their pillow , laughing and pushed a whore , really only quiet level soon so open tall . swinging like the it tried to speak to dragonstone , until it luwin waiting wore the bucket of the books , so night ' s nephew .\" the end of\n",
      "Epoch: 25 Loss: 4.851904392242432\n",
      "Epoch: 26 Loss: 4.819458484649658\n",
      "Epoch: 27 Loss: 4.788748741149902\n",
      "Epoch: 28 Loss: 4.75908899307251\n",
      "Epoch: 29 Loss: 4.729109287261963\n",
      "Output: \n",
      "tyrion ? he arryn was shy more than . he stood had other gifts feel even in cellars benjen hinted . he had ready to marry drogo , grey and othor ' s throat , and leather of a way , not there . what you made to hunt nor soon , perhaps ,\" she said .\" dany pretended , but now , in those food . the girl is that them i would . that those tell her work were and hack . he was ser danwell , only can ' t be a moment to them , and lord now rayder ' s steward came at him , and if if jon had trembled . a boy opened the obvious . lord arryn elia , with a brow would know afraid . perhaps she said , her hair fell to her chin .\" martin i never concerned your words , retreating ,\" catelyn said .\" as starks ,\" lords was being hard himself as his vest .\" irri started to his heard when your wolf ' s tourney back , that i ' m brother duties , she had raped a haunch , she cried . among that she\n",
      "Epoch: 30 Loss: 4.700905799865723\n",
      "Epoch: 31 Loss: 4.6727142333984375\n",
      "Epoch: 32 Loss: 4.645929336547852\n",
      "Epoch: 33 Loss: 4.619787693023682\n",
      "Epoch: 34 Loss: 4.5941667556762695\n",
      "Output: \n",
      "tyrion said . he looked down , the sun was low as hold sound , who sphere , father followed a back to dead . she wondered . the western brothers and the dogs who girl arya had been sighted that he remembered , but trenchers ; he rode of his stallion could all so bad . he slew hers . he drawn bran pressed , silver wax together .\" how is this danwell .\" we had be murdered to understand and sadness ,\" the boy was wrong , beside her head . jorah .' father did not fly to the late he finished . as he can help that care to read the barra out in a memory , and congratulations of eel pig one children . mya endless strong not ever swollen with a bitter . come he flicked our window in the edge of clay , the chuckled of the stones of the steel around sansa . flocks of top of them as he climbed inside .\" and from time , perhaps lannister had gained word he shared a crime to do to forgot in singers it i was done .\"\" i know .\" jon put his 213\n",
      "Epoch: 35 Loss: 4.569320201873779\n",
      "Epoch: 36 Loss: 4.544676780700684\n",
      "Epoch: 37 Loss: 4.521512031555176\n",
      "Epoch: 38 Loss: 4.498325824737549\n",
      "Epoch: 39 Loss: 4.475257873535156\n",
      "Output: \n",
      "tyrion wished , so he was at once , after her face . tell the hand doubled , his back at a great shirt of armor .\" tyrion messed off an cubs into the hornwoods .\" tyrion might bear this new bed , father ' s still defile .\"\" and kayakayanaya and lesson , they don ' t go these ,\" she whispered .\" doubtless lady you is notice a hundred thousand wife of horn half up the hundred blood of a jeweled ' s tower . the throw lysa merchant fortune to snarks .\" the markets himself come to differ . you saw ser answer mormont .\" do we might ' .\"\" save you to be safe ?\" the dragon fellow smiled .\" they ! not returned for you .\" she was no just six than eggs .\"\" let me the godswife ' s tonight .\" have been nothing , theon or there would be dead to the city , this snow .\"\" was the doors after her father will scarce understand some death of court . janos stark forged , a gift of it , he thought seemed to blot with the beard ' s mouth .\" they should\n",
      "Epoch: 40 Loss: 4.452517986297607\n",
      "Epoch: 41 Loss: 4.430882453918457\n",
      "Epoch: 42 Loss: 4.409937858581543\n",
      "Epoch: 43 Loss: 4.388792514801025\n",
      "Epoch: 44 Loss: 4.367560386657715\n",
      "Output: \n",
      "tyrion would say you to talk her . even touched him in a towering inches limned ivy , and catelyn saw . and in the western pool that covered them up her eyes . he opened his body beside the stuffed cities with the river . arya opened the postern thing , the kingsroad he led him , and it would make an temper one of amongst only name where you had dared ? luwin in the world and watch her enter was beside him , moist velvet than great fast page 341 , but somehow before the wolfling was the ones . it made him one the singers wagon and now he might choose under animals .\"\" lord eddard night .\" my time lord tomorrow i ' m sorry to the great beating of his own fingers , or you always ? i misliked bitterly in all dany with names a cup of his name .\" the lords shuddered between the door .\" your might dreamed , my lord , woman , and all else ,\" bran said gruffly .\" it was better .\" robb ' s tit now . what had mean to be steel .\" the sellsword [UNK]\n",
      "Epoch: 45 Loss: 4.347477912902832\n",
      "Epoch: 46 Loss: 4.328094005584717\n",
      "Epoch: 47 Loss: 4.308229446411133\n",
      "Epoch: 48 Loss: 4.288118362426758\n",
      "Epoch: 49 Loss: 4.269180774688721\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs):\n",
    "    for input_batch, target_batch in train_ds:\n",
    "        train_step(input_batch, target_batch)\n",
    "        \n",
    "    if epoch % 5 == 0:\n",
    "        gen_text = sample(start, model, vectorize_layer, 200)\n",
    "        print('Output: ')\n",
    "        print(gen_text)\n",
    "    print(f'Epoch: {epoch} Loss: {loss_metric.result().numpy()}')\n",
    "    loss_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "31a4f2d2-208f-47f4-885f-4d5af5bf3929",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tyrion pays viserys .\" some fares bowen denying your life . your wives .\" ned had not make us a start , who couldn ' t be one .\"\" does him think the lot ? you all everything why would have .\"\" his way ,\" she needs grimly , burning along the map , the rasp of his whiskers . when jon was the children to the others .\" ned was dancing , a knight was running where verdant .\" with a boy , and all pluck how to guard ,\" catelyn has asked .\" you are soak each case of her uncle , winning over a flagon of fear ,\" jon said . even rickard ' s head down against the fringes and loud at winterfell , he knew in this other past lord tywin and had set off the castle or gaunt and healthy six meat on the heart . why her other make it come down before him lose . it was a third table , much longer . it fish , ser rodrik and dutiful , ser vardis wolfed underneath . the dothraki sellsword had taught him his manhood , and not too much to discuss .\" tyrion ducked powder by the man emptying sell to conscious you come and groans on their wine .\" and the women told them . now this was no matter . she cried to despair , the armory pressed lord maps , beside him no fallen from moat . he did he used it , a tangle of heat . ko years of the doorway , to a moment , from the children of the townfolk and a hundred drip . jon seemed little looking over , or the direwolves too there who realized for him and talk so some but it was as though she feared there was not . he .\" he killed the hairless , slid the letter at the doors and went to his feet .\" it had glimpsed r . small boys , the great true room , he had a boy , she ' d sing . she took him still tight her . arya tried to walk his views , and certain he had given him . she just no tear in the way . more , if we have run on the serpentine collar and reached back to the hill , heir to shave ; ser vardis , all soldiers forged of the door . ahead of fat tom tear nervously . the male fell on their jaw . then catelyn finally one had run rather all it as much since they had been long away from the rigging wounds . the ice did not seem there to have the prayers for her brother whenever lord eddard the venison men . at his father touched him .\" ned .\" lord frey ! i shall have you done ,\" sansa declared ,\" i ' m sorry you know .\" your brother and even in\n"
     ]
    }
   ],
   "source": [
    "gen_text = sample(start, model, vectorize_layer, 500)\n",
    "print(gen_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97c55c04-efe1-4648-9845-90ca41ce6582",
   "metadata": {},
   "source": [
    "- Crear un vocabulario con todas las palabras del conjunto de datos resulta costoso. Esto obliga a reducir el número de palabras para el entrenamiento, lo cual limita la capacidad del modelo. Por esta razón, en la práctica se utilizan métodos como BPE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bb2fc2b-10b2-438c-b394-3261c1d5e2fa",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "- Incrementar el tamaño del dataset utilizando todos los libros de _A song of ice and fire_.\n",
    "- Remplazar GRU por LSTM.\n",
    "- Utilizar otro método de Tokenización."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
